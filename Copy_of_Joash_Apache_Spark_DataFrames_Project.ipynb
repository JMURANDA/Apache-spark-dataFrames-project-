{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "#Pre-requisites\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rBAw7nKFIzPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing pyspark\n",
        "#\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G6xIhcbI9FC",
        "outputId": "ae779552-8d0d-491f-dfbf-b6832d98cec4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we'll run a local spark session\n",
        "# ---\n",
        "#\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "sX6kUruzJg3M"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start a spark session and load the stock file while inferring the data types.\n"
      ],
      "metadata": {
        "id": "PTOcItl0R5CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/saf_stock (1).csv\", inferSchema=True, header=True)\n"
      ],
      "metadata": {
        "id": "hMwceCPlLuWy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Determine the column names"
      ],
      "metadata": {
        "id": "IsFOY7vCSIZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = df.columns\n",
        "print(columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD9UD8mASNrh",
        "outputId": "394fd20f-745d-44dc-dfec-826be7a27702"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make observations about the schema.\n",
        "\n",
        "\n",
        "Ans:The data types of the columns contain intergers,it has seven columns including date , open stock price of the day  ,high stock price of the day,  low stock price of the day, closing stock price of the day, volume that include the number shares traded per day and adjusted closing stock price per day ."
      ],
      "metadata": {
        "id": "rFnheDsjStHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the first 5 rows\n"
      ],
      "metadata": {
        "id": "tArCHChDUPJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa9GLCZ3UQ5Y",
        "outputId": "440cc99c-62f7-41de-e554-1109252022a5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
            "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06 00:00:00|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09 00:00:00|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
            "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the describe method to learn about the data frame"
      ],
      "metadata": {
        "id": "7DABr_q9Ug2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlDaLKe6U-X1",
        "outputId": "3d3e97bc-728a-4dd2-805c-556a934d2ad3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation\n"
      ],
      "metadata": {
        "id": "IIUi4NhYVrNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import format_number\n"
      ],
      "metadata": {
        "id": "ttviF1PRWDt3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format all the data to 2 decimal places i.e. format_number()"
      ],
      "metadata": {
        "id": "YwOc7J_fVyJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.columns[1:]).describe().select(\n",
        "    format_number(\"Open\",2).alias(\"Open\"),\n",
        "    format_number(\"High\",2).alias(\"High\"),\n",
        "    format_number(\"Low\",2).alias(\"Low\"),\n",
        "    format_number(\"Close\",2).alias(\"Close\")\n",
        "    format_number(\"summary\",2).alias(\"summary\")\n",
        "    format_number(\"Volume\",2).alias(\"Volume\")\n",
        "    format_number(\"Adj Close\",2).alias(\"Adj Close\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "byi3yYqbVoww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new data frame with a column called HV Ratio that is the ratio of the\n",
        "High Price versus volume of stock traded for a day"
      ],
      "metadata": {
        "id": "QcfmkKQunWqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "dJDMksUDnfGr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hv = df.withColumn(\"HV Ratio\", col(\"High\")/col(\"Volume\"))\n",
        "df_hv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM7jeo1mn55y",
        "outputId": "cbbd6cdc-b473-4462-c346-32347a036892"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: timestamp, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double, HV Ratio: double]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Analysis\n"
      ],
      "metadata": {
        "id": "DoZHnHF5oSe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What day had the Peak High in Price?\n"
      ],
      "metadata": {
        "id": "g9FEdb96oXq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n"
      ],
      "metadata": {
        "id": "VV9c3nNIo76C"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_high = df.select(max(\"High\").alias(\"High\"))\n",
        "df.select(\"Date\", \"High\").where(df[\"High\"] == df_high.collect()[0][0]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4VHWuGWpaC2",
        "outputId": "3d4225d1-0491-4cae-c6fa-89c7da0148f2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+\n",
            "|               Date|     High|\n",
            "+-------------------+---------+\n",
            "|2015-01-13 00:00:00|90.970001|\n",
            "+-------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the mean of the Close column?\n"
      ],
      "metadata": {
        "id": "JMUKtKouptEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "df.select(mean(\"Close\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXByXn8lp8HT",
        "outputId": "6dbf0271-a55f-47c7-ec6c-ac9caf93d6d5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844998012726|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max and min of the Volume column?\n"
      ],
      "metadata": {
        "id": "gUTB9CarqSID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, min\n",
        "df.select(max(\"Volume\"), min(\"Volume\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFi5fhCrqWSa",
        "outputId": "8dc9e8ce-3278-4ebc-88ab-550c63c9365c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many days was the Close lower than 60 dollars?"
      ],
      "metadata": {
        "id": "m64aWoqkqumb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "df.filter(df[\"Close\"] < 60).select(count(\"Close\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSJJAIlyq2rV",
        "outputId": "4c6fe6fe-1260-468b-e53c-e43cf3bebaf2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|count(Close)|\n",
            "+------------+\n",
            "|          81|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What percentage of the time was the High greater than 80 dollars?\n"
      ],
      "metadata": {
        "id": "QSsrf6SBrCUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, lit\n",
        "\n",
        "total = df.select(count(\"High\")).collect()[0][0]\n",
        "greater_than_80 = df.filter(df[\"High\"] > 80).select(count(\"High\")).collect()[0][0]\n",
        "percentage = (greater_than_80 / total) * 100\n",
        "print(str(round(percentage, 2)) + \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESnyvCgorGfP",
        "outputId": "3e08d4cd-53f8-49f0-f409-8ce3cbce613f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the Pearson correlation between High and Volume?\n"
      ],
      "metadata": {
        "id": "a2z9tSlKrd5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr\n",
        "\n",
        "df.select(corr(\"High\", \"Volume\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUSxI4HWriRk",
        "outputId": "e93280de-0ce8-4607-8fd6-b4f529c1c40b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "| corr(High, Volume)|\n",
            "+-------------------+\n",
            "|-0.3384326061737161|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max High per year?\n"
      ],
      "metadata": {
        "id": "t_rxQ8Bor325"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, max\n",
        "\n",
        "df.groupBy(year(\"Date\").alias(\"Year\")).agg(max(\"High\")).orderBy(\"Year\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUEFNQImr6Ur",
        "outputId": "fc406c8d-fefe-4fac-8b07-7372e7506ffc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2012|77.599998|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2015|90.970001|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the average Close for each Calendar Month?"
      ],
      "metadata": {
        "id": "Kv34l5OyswAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, avg\n",
        "\n",
        "df.groupBy(month(\"Date\").alias(\"Month\")).agg(avg(\"Close\")).orderBy(\"Month\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijKw-QNEs7zq",
        "outputId": "d282732a-1675-4687-e782-4dfde612dddc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|    1|71.44801958415842|\n",
            "|    2|  71.306804443299|\n",
            "|    3|71.77794377570092|\n",
            "|    4|72.97361900952382|\n",
            "|    5|72.30971688679247|\n",
            "|    6| 72.4953774245283|\n",
            "|    7|74.43971943925233|\n",
            "|    8|73.02981855454546|\n",
            "|    9|72.18411785294116|\n",
            "|   10|71.57854545454543|\n",
            "|   11| 72.1110893069307|\n",
            "|   12|72.84792478301885|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}